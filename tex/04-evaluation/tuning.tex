\documentclass[../../report.tex]{subfiles}
\begin{document}
\section{Hyperparameter Tuning}

In order to ensure a fair comparison between the two different model
architectures, it is imperative to ensure that the values of hyperparameters are
optimal. Generally, this is a non-trivial task, and in research projects it
often accounts for the vast majority of compute time \cite{Strubell2019}. Given
the hardware limitations in this project, hyperparameter search was restricted
to a handful of informed guesses, as opposed to a more systematic approach (e.g.
grid search).

\subsection{Experiments}

After analysing successful LSNN configurations described in literature
\cite{Bellec2018LSNN, Bellec2020}, an candidate set of hyperparameters was
devised (table \ref{tab:lsnn-base}). Then, several experiments were performed by
perturbing individual parameters in isolation (table
\ref{tab:lsnn-experiments}).

\begin{table}
  \begin{center}
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{ r | r | l }
      Parameter & Value & Description
      \\ \hline
      \(B\) & 128 & Batch size
      \\
      \(\eta\) & 0.001 & Learning rate
      \\
      \(\gamma\) & 0.3 & Pseudo-derivative dampening factor
      \\
      \(N\) & (512) & Neurons per layer (tuple)
      \\
      \(n_\mathrm{in}\) & 1 & Neurons per input class
      \\
      \(n_\alpha\) & 0.4 & Fraction of neurons with adaptation
      \\
      \(\delta t\) & 1.0 & Time step (ms)
      \\
      \(t_r\) & 2 & Number of refractory time steps
      \\
      \(t_\mathrm{in}\) & 5 & Number of time steps per input event
      \\
      \(\tau_\mathrm{out}\) & 3.0 & Output readout time constant (ms)
      \\
      \(\tau_v\) & 20.0 & Membrane potential time constant (ms)
      \\
      \(\tau_\alpha\) & 500.0 & Neuronal adaptation time constant (ms)
      \\
      \(\alpha\) & 0.1 & Neuronal adaptation magnitude
      \\
      \(v_\mathrm{thr}\) & 1.0 & Threshold membrane potential
      \\
      \(f_\mathrm{in}\) & 1000 & Input firing rate (Hz)
      \\
      \(f_\mathrm{reg}\) & 20 & Firing rate regularisation target (Hz)
      \\
    \end{tabular}
  \end{center}
  \caption{Base LSNN configuration}
  \label{tab:lsnn-base}
\end{table}

\begin{table}
  \renewcommand{\arraystretch}{1.25}
  \begin{tabularx}{\textwidth}{ l | l | >{\raggedright\arraybackslash}X }
    Changes & Identifier & Rationale/Hypothesis \\
    \hline
    \(N = (304, 304)\) &
    \texttt{lsnn-layered} &
    Two-layer configuration with the same number of trainable weights. \\
    \hline
    \(
    \begin{aligned}[t]
      B &= 32 \\
      t_\mathrm{in} &= 20 \\
      \tau_\alpha &= 2000.0 \\
      f_\mathrm{in} &= 250 \\
    \end{aligned}
    \) &
    \texttt{lsnn-longinput} &
    Longer input presentation period with a reduced input spike rate. Batch size
    and adaptation time constant were adjusted to compensate. \\
    \hline
    \(f_\mathrm{reg} = 10\) &
    \texttt{lsnn-quiet} &
    Testing the effect of a lower target firing rate. \\
    \hline
    \(\tau_\alpha = 250.0\) &
    \texttt{lsnn-forgetful} &
    A shorter adaptation time constant should reduce ability to capture
    long-term dependencies. \\
    \hline
    \(\alpha = 0.2\) &
    \texttt{lsnn-rigid} &
    Testing the effect of a steeper adaptation curve. \\
    \hline
    \(n_\alpha = 0.8\) &
    \texttt{lsnn-alif} &
    A model with more adaptive neurons might learn better. \\
    \hline
    \(n_\alpha = 0.0\) &
    \texttt{lif} &
    A model without adaptive neurons should perform considerably worse. \\
  \end{tabularx}
  \caption{Experimental LSNN configurations}
  \label{tab:lsnn-experiments}
\end{table}

\subsection{Results}

All experimental configurations were trained for \num{3000} steps. To maximise
training speed, testing on the validation dataset was only performed every
\num{1000} steps. Finally, the configuration with the highest event prediction
accuracy was chosen, which turned out to be \texttt{lsnn-rigid}. For the
complete set of results, see figure \ref{fig:lsnn-eval}.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={Training step},
      ylabel={Event accuracy},
      legend style={
        cells={anchor=west},
        legend pos=outer north east,
      }
    ]
      \legend{
        \texttt{lsnn-base},
        \texttt{lsnn-layered},
        \texttt{lsnn-longinput},
        \texttt{lsnn-quiet},
        \texttt{lsnn-forgetful},
        \texttt{lsnn-rigid},
        \texttt{lsnn-alif},
        \texttt{lsnn-lif},
      }
      \addplot table [
        col sep=comma,
        x=Step,
        y=Value,
      ]{\subfix{csv/eval-base.csv}};
      \addplot table [
        col sep=comma,
        x=Step,
        y=Value,
      ]{\subfix{csv/eval-layered.csv}};
      \addplot table [
        col sep=comma,
        x=Step,
        y=Value,
      ]{\subfix{csv/eval-longinput.csv}};
      \addplot table [
        col sep=comma,
        x=Step,
        y=Value,
      ]{\subfix{csv/eval-quiet.csv}};
      \addplot table [
        col sep=comma,
        x=Step,
        y=Value,
      ]{\subfix{csv/eval-forgetful.csv}};
      \addplot table [
        col sep=comma,
        x=Step,
        y=Value,
      ]{\subfix{csv/eval-rigid.csv}};
      \addplot table [
        col sep=comma,
        x=Step,
        y=Value,
      ]{\subfix{csv/eval-alif.csv}};
      \addplot table [
        col sep=comma,
        x=Step,
        y=Value,
      ]{\subfix{csv/eval-lif.csv}};
    \end{axis}
  \end{tikzpicture}
  \caption{Experimental configurations' accuracy on the validation set}
  \label{fig:lsnn-eval}
\end{figure}

\end{document}
