\documentclass[../../report.tex]{subfiles}
\begin{document}
\section{Comparison}

The optimal set of hyperparameters determined in the previous section shall now
be used to compare the LSNN model against the baseline LSTM model.

\subsection{LSTM configuration}

Although LSTM and LSNN are vastly different models under the hood, one way to
make the comparison fair is by equalising the number of trainable weights
between the two models. The functions below give the number of weights for
LSTM, LSNN, and linear layers with \(n\) inputs and \(m\) outputs.

\begin{align*}
  W_\mathrm{LSTM}(n, m) &= 4 \cdot (nm + m^2 + m)
  \\
  W_\mathrm{LSNN}(n, m) &= nm + m^2
  \\
  W_\mathrm{linear}(n, m) &= nm
\end{align*}

The total number of weights in the chosen LSNN configuration is:

\begin{equation*}
  n = W_\mathrm{LSNN}(38, 512) + W_\mathrm{linear}(512, 38) = \num{301056}
\end{equation*}

The default configuration of Basic RNN uses two LSTM layers of 128 units each,
as this has been found to give good results. For the comparison, the layer sizes
shall be modified such that the number of weights becomes as close to \(n\) as
possible. The optimal layer size \(x\) can be determined by solving the
following equation:

\begin{align*}
  n &= W_\mathrm{LSTM}(38, x) + W_\mathrm{LSTM}(x, x) + W_\mathrm{linear}(x, 38) \\
  x &\approx 150 \\
\end{align*}

Therefore, the layer sizes will be set to \((150, 150)\) for the comparison. All
other parameters of the LSTM remain unchanged.

\subsection{Results}

Both models were trained for \num{10000} steps, while evaluating performance on
the validation melodies every \num{1000} steps. The results show that while the
LSNN takes the lead initially, it is soon overtaken by the LSTM. At the end of
training, it is clear that the LSTM can achieve a better event accuracy,
although its counterpart is not far behind. Interestingly, the LSNN achieves a
slightly better per-class accuracy.

A more pressing issue is the fact that the LSNN is an order of magnitude slower
in training. This is not very surprising, as the LSTM consists of simple
operations that can be massively accelerated by the GPU. On the other hand, as
the LSNN models real biological phenomena, its step computation is considerably
more convoluted.

\missingfigure{Result graphs}

\end{document}
