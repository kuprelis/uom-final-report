\documentclass[../../report.tex]{subfiles}
\begin{document}
\section{Model Architecture}

Understanding and implementing the LSNN was one of the major challenges in this
project. It was very helpful to analyse the experimental code published by
researchers at TU Graz \cite{Bellec2018LSNN, Bellec2020}. Still, large parts of
their implementation had to be rewritten in order to make the LSNN interoperable
with Magenta, and to improve code readability.

\subsection{LSNN cell}

TensorFlow comes bundled with many standard neural network components, e.g.
LSTM. Of course, esoteric modules such as the LSNN must be implemented manually.
In TensorFlow, custom recurrent networks are defined by extending the abstract
\texttt{RNNCell} class, and our LSNN is no exception.

\subsubsection{Constructor arguments}
The LSNN cell has many hyperparameters. Their optimal values depend on the task,
so they have been made configurable via the constructor.

\todo[inline]{Turn this into a table}
\begin{itemize}
  \item num neurons -- the number of neurons modeled by this cell
  \item dt -- the physical time step represented by a single iteration
  \item frac alif -- fraction of neurons with adaptive firing thresholds
  \item num refractory dt -- length of the refractory period
  \item spike threshold -- base firing threshold
  \item potential decay -- membrane potential time constant \todo{define}
  \item adaptation decay -- adaptive threshold time constant \todo{define}
  \item adaptation magnitude -- adaptive threshold scaling factor
  \item dampening factor -- psuedo-derivative scaling factor \todo{training link}
\end{itemize}

\subsubsection{State}
The state of an LSNN cell is a tuple of six tensors, as described below:

\todo[inline]{Turn this into a table}
\begin{itemize}
  \item potentials -- membrane potential of each neuron
  \item adaptations -- magnitude of threshold adaptation for each neuron
  \item spikes -- binary vector indicating neurons that spiked in the previous step
  \item refractoriness -- refractory period counter for each neuron
  \item num\_spikes -- running total of spikes for each neuron
  \item num\_steps -- running total of steps taken by this cell (scalar)
\end{itemize}

\subsubsection{Input \& output}

The LSNN cell expects to receive binary input spike vectors of arbitrary but
constant size. The output is a binary vector of spikes from internal LSNN
neurons.

\subsubsection{Operation}

Subclasses of \texttt{RNNCell} must implement the \texttt{call} method. It
should take the input and state as arguments, and it should return the output
and next state. The LSNN cell achieves this as follows:

\todo[inline]{Consider turning this into pseudocode}
\todo[inline]{Consider adding equations}
\begin{enumerate}

  \item Input spikes and internal spikes are multiplied with corresponding
  synaptic weight matrices; the results are added to membrane potentials of
  internal neurons.

  \item Membrane potentials and adaptive thresholds are used to determine which
  neurons will spike at this time step.

  \item Spikes are blocked for neurons currently in the refractory period.

  \item Decay factor is applied to membrane potentials.

  \item Decay factor is applied to adaptive thresholds.

  \item Nonzero refractory counters are decremented.

  \item Membrane potentials are reset for neurons spiking in this step.

  \item Adaptive thresholds are increased for ALIF neurons that spiked in this
  step.

  \item Refractory counters are initialised for neurons spiking in this step.

  \item New state vectors are packed into a tuple and returned along with
  internal spikes.

\end{enumerate}

\subsection{Smoothing the outputs}

\subsection{Spiking the inputs}

\subsection{Putting it all together}

\end{document}
