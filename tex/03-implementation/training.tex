\documentclass[../../report.tex]{subfiles}
\begin{document}
\section{Training}

The model is optimised by repeating a simple training loop comprised of several
steps:

\begin{enumerate}
  \item A batch of melodies is randomly sampled from the training set.

  \item The melody batch is fed as an input to the model.

  \item Predicted events are compared against the true labels to calculate the
  loss.

  \item Model parameters are adjusted according to the loss gradient.
\end{enumerate}

\subsection{Loss}

The final layer in the model produces an output vector \(\bm{y}\) containing 38
activation values, one for each possible melody event. Note that the magnitude
of these values is unbounded, therefore a probability distribution
\(\bm{\hat{p}}\) is computed using the softmax function:

\begin{equation*}
  \hat{p}_i = \frac{ \exp(y_i) }{ \sum_{j} \exp(y_j) }
\end{equation*}

As is common in classification tasks, the accuracy of the estimated probability
distribution \(\bm{\hat{p}}\) is quantified against the ground truth \(\bm{p}\)
using \emph{cross-entropy} loss:

\begin{equation*}
  L(\bm{p}, \bm{\hat{p}}) = - \bm{p} \cdot \log(\bm{\hat{p}})
\end{equation*}

In fact, since \(\bm{p}\) is effectively a one-hot vector, the loss is simply
the negated log probability of the ground truth label.

The per-batch loss value is obtained by averaging the loss across all individual
predictions for the melodies in that batch.

\subsection{LSNN details}

Swapping the LSTM for an LSNN introduces additional considerations to the
training procedure.

\subsubsection{Custom gradient}
Formally, the value of a neuron's spike function \(s\) at time \(t\) is defined
by the Heaviside step function.

\begin{align*}
  s_t &= H(v_t - v_\mathrm{thr})
  \\
  H(x) &=
  \begin{cases}
  1 & \text{if } x \geq 0 \\
  0 & \text{if } x < 0
  \end{cases}
\end{align*}

In other words, the value is 1 if the membrane voltage \(v_t\) is greater than
or equal to firing threshold \(v_\mathrm{thr}\), and 0 otherwise. Being
piecewise constant, the spike function is unsuitable for backpropagation.
Therefore, it is necessary to define a pseudo-derivative \(\frac{ds}{dv}\) that
can be combined with the upstream loss gradient \(\frac{dL}{ds}\).

\begin{align*}
  v_\mathrm{scaled} &= \frac{v}{v_\mathrm{thr}} - 1
  \\
  \frac{ds}{dv} &= \max(0, 1 - \left| v_\mathrm{scaled} \right|) \cdot \gamma
  \\
  \frac{dL}{dv} &= \frac{ds}{dv} \cdot \frac{dL}{ds}
\end{align*}

The derivative is scaled by a dampening factor \(\gamma = 0.3\). This stabilises
learning by preventing erratic loss gradients (figure \ref{fig:dampening}).

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={Training step},
      ylabel={Gradient norm},
      legend entries={\(\gamma=0.3\), \(\gamma=0.0\)},
      legend style={at={(0.02,0.98)}, anchor=north west},
    ]
      \addplot table [
        col sep=comma,
        x=Step,
        y=Value,
      ]{\subfix{csv/gradient-dampened.csv}};
      \addplot table [
        col sep=comma,
        x=Step,
        y=Value,
      ]{\subfix{csv/gradient-undampened.csv}};
    \end{axis}
  \end{tikzpicture}
  \caption{The dampening factor \(\gamma\) ensures a stable gradient}
  \label{fig:dampening}
\end{figure}

\subsubsection{Firing rate regularisation}

To prevent biologically unrealistic firing rates, deviations from the target
firing frequency \(f_\mathrm{target}\) are penalised using an additional square
term in the loss function.

\begin{equation*}
  L_\mathrm{rate} = \frac{1}{2} (f - f_\mathrm{target})^2
\end{equation*}

The firing rate \(f\) is calculated for each neuron individually by averaging
the value of its spike function across all time steps in the sequence.

\subsection{Hardware limitations}

All models were trained using an Nvidia GeForce 960M GPU with only 2 gigabytes
of memory, which imposed hard limits on the training procedure.

During the first few training attempts, it became clear that long melodies are
problematic. When feeding inputs into the model, TensorFlow retains all
intermediate RNN states in memory, so that they can later be used to compute the
loss gradient. This can cause out-of-memory errors with longer sequences. To
stay within the memory budget, all melodies were truncated to a maximum length
of 128 steps.

Almost all model configurations were trained on batches of 128 melodies. Batch
size is also a major factor in memory usage, but it was crucial not to
compromise in this regard. Melodies are highly variable, so the hope is that
averaging the loss across a large batch prevents the model from overfitting to
the quirks of a single melody. Instead, the model is forced to learn general
patterns.

Training was also attempted on a Google Cloud Platform virtual machine equipped
with a 16-gigabyte Nvidia Tesla T4 GPU. This enabled training larger models, but
no speedup was observed. The core issue is that RNN computations are
intrinsically non-parallelisable, meaning that the main performance bottleneck
was likely the sheer length of the training sequences.

\end{document}
